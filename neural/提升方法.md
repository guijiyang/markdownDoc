# 提升方法
提升方法是一种常用的统计学习方法，应用广泛且有效。再分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

## 提升方法AdaBoost算法
历史上首先提出了‘强可学习’和‘弱可学习’的概念。在概率近似正确（PAC）学习的框架下，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。后来有人证明抢课学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。
对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则容易得多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。
AdaBoost提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器‘分而治之’。同时，AdaBoost采取加权多数表决的方法。加大分类误差率小的弱分类器的权值，使其再表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。

### AdaBoost算法
假设给定一个二类分类的训练数据集$$T=\lbrace (x_1,y_1),(x_2,y_2),...,(x_N,y_N)\rbrace$$其中，每个样本点由实例与标记组成。实例$x_i\in \chi\subseteq R^n$，标记$y_i\in y=\lbrace -1,+1\rbrace$，$\chi$是实例空间，$y$是标记集合。

AdaBoost算法：
输入：训练数据集$T=\lbrace (x_1,y_1),(x_2,y_2),...,(x_N,y_N)\rbrace$；弱学习算法；
输出：最终分类器$G(x)$。

- 初始化训练数据的权值分布$$D_1=(w_{11},\dots,w_{1i},\dots,w_{1N}), w_{1i}=\frac{1}{N},i=1,2,\dots,N$$
- 对$m=1,2,\dots,M$
    - 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器$$G_m(x):\chi\to\lbrace -1,+1\rbrace$$
    - 计算$G_m(x)$再训练数据集上的分类误差率$$e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\neq y_i)$$
    - 计算$G_m(x)$的系数$$\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}$$这里的对数是自然对数。
    - 更新训练数据集的权值分布$$D_{m+1}=(w_{m+1,1},\dots,w_{m+1,i},\dots,w_{m+1,N})$$,$$w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),i=1,2,\dots,N$$这里，$Z_m$是规范化因子$$Z_m=\sum_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x))$$它使D_{m+1}成为一个概率分布。
- 构建基本分类器的线性组合$$f(x)=\sum_{m=1}^M\alpha_mG_m(x)$$得到最终分类器$$G(x)=sign(f(x))=sign\left(\sum_{m=1}^m\alpha_mG_m(x)\right)$$

对AdaBoost算法作如下说明：
- 1. 假设训练数据集具有均匀的权值分布，即每一个训练样本在基本分类器的学习中作用相同，这一假设保证能够再原始数据上学习基本分类器$G_1(x)$。
- 2. AdaBoost反复学习基本分类器，在每一轮$m=1,2,\dots,M$顺序执行下列操作：
    - a. 使用当前分布$D_m$加权的训练数据集，学习基本分类器$G_m(x)$.
    - b. 计算基本分类器$G_m(x)$在加权训练数据集上的分类误差率:$$e_m=P(G_m(x_i)\neq y_i)=\sum_{G_m(x_i\neq y_i)}w_{mi}$$这里，$w_{mi}$表示第$m$轮中第i个实例的权值，$\sum_{i=1}^Nw_{mi}=1$。这表明，$G_m(x)$在加权的训练数据集上的分类误差率是被$G_m(x)$误分类样本的权值之和，由此可以看出数据权值分布$D_m$与基本分类器$G_m(x)$的分类误差率的关系。
    - c. 计算基本分类器$G_m(x)$的系数$\alpha_m$。$\alpha_m$表示$G_m(x)$在最终分类器中的重要性。由$\alpha_m$的计算公式可知，当$e_m\leq\frac{1}{2}$时，$\alpha_m\geq0$，并且$\alpha_m$随着$e_m$的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。
    - d. 更新训练数据的权值分布为下一轮作准备。
    $$w_{m+1,i} =
    \begin{cases}
    \frac{w_{mi}}{Z_m}e^{-\alpha_m}, & G_m(x_i)=y_i \\
    \frac{w_{mi}}{Z_m}e^{\alpha_m},&  G_m(x_i)\neq y_i 
    \end{cases}$$
由此可知，被基本分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这个AdaBoost的一个特点。
线性组合$f(x)$实现M个基本分类器的加权表决。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。
