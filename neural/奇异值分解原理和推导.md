# 奇异值分解（SVD）原理详解和推导

    在网上看到有很多文章介绍 SVD 的，讲的也都不错，但是感觉还是有需要补充的，特别是关于矩阵和映射之间的对应关系。前段时间看了国外的一篇文章，叫 A Singularly Valuable Decomposition The SVD of a Matrix，觉得分析的特别好，把矩阵和空间关系对应了起来。本文就参考了该文并结合矩阵的相关知识把 SVD 原理梳理一下。

   SVD 不仅是一个数学问题，在工程应用中的很多地方都有它的身影，比如前面讲的 PCA，掌握了 SVD 原理后再去看 PCA 那是相当简单的，在推荐系统方面，SVD 更是名声大噪，将它应用于推荐系统的是 Netflix 大奖的获得者 Koren，可以在 Google 上找到他写的文章；用 SVD 可以很容易得到任意矩阵的满秩分解，用满秩分解可以对数据做压缩。可以用 SVD 来证明对任意 M*N 的矩阵均存在如下分解：

![](https://img-blog.csdn.net/20150123160014873?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

这个可以应用在数据降维压缩上！在数据相关性特别大的情况下存储 X 和 Y 矩阵比存储 A 矩阵占用空间更小！

   在开始讲解 SVD 之前，先补充一点矩阵代数的相关知识。

正交矩阵
----

   正交矩阵是在欧几里得空间里的叫法，在酉空间里叫酉矩阵，一个正交矩阵对应的变换叫正交变换，这个变换的特点是不改变向量的尺寸和向量间的夹角，那么它到底是个什么样的变换呢？看下面这张图

![](https://img-blog.csdn.net/20150123124108372?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

假设二维空间中的一个向量 OA，它在标准坐标系也即 e1、e2 表示的坐标是中表示为 (a,b)'（用'表示转置），现在把它用另一组坐标 e1'、e2'表示为 (a',b')'，存在矩阵 U 使得 (a',b')'=U(a,b)'，则 U 即为正交矩阵。从图中可以看到，正交变换只是将变换向量用另一组正交基表示，在这个过程中并没有对向量做拉伸，也不改变向量的空间位置，加入对两个向量同时做正交变换，那么变换前后这两个向量的夹角显然不会改变。上面的例子只是正交变换的一个方面，即旋转变换，可以把 e1'、e2'坐标系看做是 e1、e2 坐标系经过旋转某个斯塔角度得到，怎么样得到该旋转矩阵 U 呢？如下

                                                                                       ![](https://img-blog.csdn.net/20150123135846821)

                                                                  ![](https://img-blog.csdn.net/20150123135825015)

                                                                   ![](https://img-blog.csdn.net/20150123135926913)

a'和 b'实际上是 x 在 e1'和 e2'轴上的投影大小，所以直接做内积可得，then

                                                                   ![](https://img-blog.csdn.net/20150123140252265)                                        

从图中可以看到

                        ![](https://img-blog.csdn.net/20150123131646169)![](https://img-blog.csdn.net/20150123131621812)  

所以

                                     ![](https://img-blog.csdn.net/20150123140255328)  

正交阵 U 行（列）向量之间都是单位正交向量。上面求得的是一个旋转矩阵，它对向量做旋转变换！也许你会有疑问：刚才不是说向量空间位置不变吗？怎么现在又说它被旋转了？对的，这两个并没有冲突，说空间位置不变是绝对的，但是坐标是相对的，加入你站在 e1 上看 OA，随着 e1 旋转到 e1'，看 OA 的位置就会改变。如下图：

![](https://img-blog.csdn.net/20150123142158281?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhvbmdrZWppbmd3YW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  

如图，如果我选择了 e1'、e2'作为新的标准坐标系，那么在新坐标系中 OA（原标准坐标系的表示）就变成了 OA'，这样看来就好像坐标系不动，把 OA 往顺时针方向旋转了 “斯塔” 角度，这个操作实现起来很简单：将变换后的向量坐标仍然表示在当前坐标系中。

旋转变换是正交变换的一个方面，这个挺有用的，比如在开发中需要实现某种旋转效果，直接可以用旋转变换实现。正交变换的另一个方面是反射变换，也即 e1'的方向与图中方向相反，这个不再讨论。

总结：正交矩阵的行（列）向量都是两两正交的单位向量，正交矩阵对应的变换为正交变换，它有两种表现：旋转和反射。正交矩阵将标准正交基映射为标准正交基（即图中从 e1、e2 到 e1'、e2'）

特征值分解——EVD
----------

    在讨论 SVD 之前先讨论矩阵的特征值分解（EVD），在这里，选择一种特殊的矩阵——对称阵（酉空间中叫 hermite 矩阵即厄米阵）。对称阵有一个很优美的性质：它总能相似对角化，对称阵不同特征值对应的特征向量两两正交。一个矩阵能相似对角化即说明其特征子空间即为其列空间，若不能对角化则其特征子空间为列空间的子空间。现在假设存在 mxm 的满秩对称矩阵 A，它有 m 个不同的特征值，设特征值为

                                                                                                   ![](https://img-blog.csdn.net/20150123145748984)

对应的单位特征向量为

                                                                ![](https://img-blog.csdn.net/20150123145830171)  

则有

                                                                 ![](https://img-blog.csdn.net/20150123150002344)  

进而

                                                               ![](https://img-blog.csdn.net/20150123150119033)  

                                                  ![](https://img-blog.csdn.net/20150123150331493)  

                                                    ![](https://img-blog.csdn.net/20150123150505140)  

所以可得到 A 的特征值分解（由于对称阵特征向量两两正交，所以 U 为正交阵，正交阵的逆矩阵等于其转置）

                                                        ![](https://img-blog.csdn.net/20150123150757364)  

这里假设 A 有 m 个不同的特征值，实际上，只要 A 是对称阵其均有如上分解。

矩阵 A 分解了，相应的，其对应的映射也分解为三个映射。现在假设有 x 向量，用Ａ将其变换到Ａ的列空间中，那么首先由 U'先对 x 做变换：

                                                                              ![](https://img-blog.csdn.net/20150123151414245)  

U 是正交阵 U'也是正交阵，所以 U'对 x 的变换是正交变换，它将 x 用新的坐标系来表示，这个坐标系就是 A 的所有正交的特征向量构成的坐标系。比如将 x 用 A 的所有特征向量表示为：

                                         ![](https://img-blog.csdn.net/20150123151915529)  

则通过第一个变换就可以把 x 表示为 [a1 a2 ... am]'：

                           ![](https://img-blog.csdn.net/20150123152046383)

紧接着，在新的坐标系表示下，由中间那个对角矩阵对新的向量坐标换，其结果就是将向量往各个轴方向拉伸或压缩：

                              ![](https://img-blog.csdn.net/20150123152331057)  

从上图可以看到，如果 A 不是满秩的话，那么就是说对角阵的对角线上元素存在 0，这时候就会导致维度退化，这样就会使映射后的向量落入 m 维空间的子空间中。

最后一个变换就是 U 对拉伸或压缩后的向量做变换，由于 U 和 U'是互为逆矩阵，所以 U 变换是 U'变换的逆变换。

因此，从对称阵的分解对应的映射分解来分析一个矩阵的变换特点是非常直观的。假设对称阵特征值全为 1 那么显然它就是单位阵，如果对称阵的特征值有个别是 0 其他全是 1，那么它就是一个正交投影矩阵，它将 m 维向量投影到它的列空间中。

根据对称阵 A 的特征向量，如果 A 是 2*2 的，那么就可以在二维平面中找到这样一个矩形，是的这个矩形经过 A 变换后还是矩形：

                                      ![](https://img-blog.csdn.net/20150123155218277)  

这个矩形的选择就是让其边都落在 A 的特征向量方向上，如果选择其他矩形的话变换后的图形就不是矩形了！

奇异值分解——SVD
----------

   上面的特征值分解的 A 矩阵是对称阵，根据 EVD 可以找到一个（超）矩形使得变换后还是（超）矩形，也即 A 可以将一组正交基映射到另一组正交基！那么现在来分析：对任意 M*N 的矩阵，能否找到一组正交基使得经过它变换后还是正交基？答案是肯定的，它就是 SVD 分解的精髓所在。

   现在假设存在 M*N 矩阵 A，事实上，A 矩阵将 n 维空间中的向量映射到 k（k<=m）维空间中，k=Rank(A)。现在的目标就是：在 n 维空间中找一组正交基，使得经过 A 变换后还是正交的。假设已经找到这样一组正交基：

                                                               ![](https://img-blog.csdn.net/20150123160515876)  

则 A 矩阵将这组基映射为：

                                                            ![](https://img-blog.csdn.net/20150123160626263)  

如果要使他们两两正交，即

                                      ![](https://img-blog.csdn.net/20150123160744762)  

根据假设，存在

                                      ![](https://img-blog.csdn.net/20150123160916671)  

所以如果正交基 v 选择为 A'A 的特征向量的话，由于 A'A 是对称阵，v 之间两两正交，那么

                                                  ![](https://img-blog.csdn.net/20150123161147171)  

这样就找到了正交基使其映射后还是正交基了，现在，将映射后的正交基单位化：

因为

                  ![](https://img-blog.csdn.net/20150123161911890)  

所以有

                                ![](https://img-blog.csdn.net/20150123162005218)  

所以取单位向量

                                        ![](https://img-blog.csdn.net/20150123162032674)  

由此可得

                ![](https://img-blog.csdn.net/20150123162324773)  

当 k <i <= m 时，对 u1，u2，...，uk 进行扩展 u(k+1),...,um，使得 u1，u2，...，um 为 m 维空间中的一组正交基，即

![](https://img-blog.csdn.net/20150123162811221)  

同样的，对 v1，v2，...，vk 进行扩展 v(k+1),...,vn（这 n-k 个向量存在于 A 的零空间中，即 Ax=0 的解空间的基），使得 v1，v2，...，vn 为 n 维空间中的一组正交基，即

![](https://img-blog.csdn.net/20150123202328388)  

则可得到

![](https://img-blog.csdn.net/20150123165814334)  

继而可以得到 A 矩阵的奇异值分解：

                                                 ![](https://img-blog.csdn.net/20150123170018218)  

                ![](https://img-blog.csdn.net/20150123170122018)  

现在可以来对 A 矩阵的映射过程进行分析了：如果在 n 维空间中找到一个（超）矩形，其边都落在 A'A 的特征向量的方向上，那么经过 A 变换后的形状仍然为（超）矩形！

vi 为 A'A 的特征向量，称为 A 的右奇异向量，ui=Avi 实际上为 AA'的特征向量，称为 A 的左奇异向量。下面利用 SVD 证明文章一开始的满秩分解：

![](https://img-blog.csdn.net/20150123170421531)  

利用矩阵分块乘法展开得：

![](https://img-blog.csdn.net/20150123171001047)  

可以看到第二项为 0，有

                               ![](https://img-blog.csdn.net/20150123171137580)  

令

![](https://img-blog.csdn.net/20150123171202821)  

            ![](https://img-blog.csdn.net/20150123171215379)  

则 A=XY 即是 A 的满秩分解。

整个 SVD 的推导过程就是这样，后面会介绍 SVD 在推荐系统中的具体应用，也就是复现 Koren 论文中的算法以及其推导过程。

参考文献：[A Singularly Valuable Decomposition The SVD of a Matrix](http://www-users.math.umn.edu/~lerman/math5467/svd.pdf)