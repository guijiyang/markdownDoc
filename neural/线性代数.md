<!-- @import "../my-style.less" -->

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1 线性代数](#1-线性代数)
  - [1.1 标量、向量、矩阵、张量](#11-标量-向量-矩阵-张量)
  - [1.2 矩阵和向量相乘](#12-矩阵和向量相乘)
  - [1.3 单位矩阵和逆矩阵](#13-单位矩阵和逆矩阵)
  - [1.4 范数](#14-范数)
  - [1.5 特殊类型的矩阵和向量](#15-特殊类型的矩阵和向量)
  - [1.6 特征分解（EVD）](#16-特征分解evd)
  - [1.7 奇异值分解（SVD）](#17-奇异值分解svd)
  - [1.8 Moore-Penrose伪逆](#18-moore-penrose伪逆)
  - [1.9 迹运算](#19-迹运算)
  - [1.10 主成分分析](#110-主成分分析)

<!-- /code_chunk_output -->

# 1 线性代数

## 1.1 标量、向量、矩阵、张量
- 标量(scalar)：一个标量就是一个单独的数。在定义实数标量时，我们可能会说"令$s\in \R$表示一条线的斜率"；在定义自然数标量时，我们可能会说"令$b\in \N$表示元素的数目"。
- 向量(vector):一个向量就是一列有序的数。如果列中的每个元素都属于$\R$，并且该向量有n个元素，那么该向量属于实数集$\R$的n次笛卡尔乘积构成的集合，记为$\R^n$。
- 矩阵(matrix):矩阵是一个二维数组。如果一个实数矩阵$A$高为m,宽度为n，那么我们说$A\in\R^{m\times n}$。
- 张量(tensor):坐标超过二维的数组。

**转置**(transpose) 是矩阵的重要操作之一。矩阵的转置是以对角线为轴的的镜像。转置具有如下性质：$$(A^T)_{i,j}=A_{j,i}$$
```python
#! /usr/bin/python3
# -*- coding:utf8 -*-
import numpy as np
A=np.linspace(-1,1,10,dtype=np.float32)
A=A.reshape((2,5))
print(A)
print(A.T)
print(A.transpose()))
```
在深度学习中，我们通常使用不那么常规的符号。我们允许矩阵和向量相加产生另一个矩阵。这涉及到隐式地复制向量生成维度相同的矩阵，这被称之为广播(broadcast)。

## 1.2 矩阵和向量相乘

两个矩阵$A$和$B$相乘需要满足$A$的列数等于$B$的行数，且生成的矩阵$C$的形状是$A$的行和$B$的列，记为：$$C=AB$$如果两个矩阵按元素相乘(element-wise product)，则记为：$$C=A\odot B$$这时必须满足$A$、$B$形状相同。
现在有如下的线性方程组：$$Ax=b\tag{1}$$其中$A\in\R^{m\times n}$是一个已知矩阵，$b\in\R^m$是已知向量，$x\in\R^n$是一个我们需要求解的未知向量。

## 1.3 单位矩阵和逆矩阵

线性代数提供了称为矩阵逆(matrix inversion)的方法，对于大多数矩阵$A$，都能通过矩阵逆求解式(1)。
任意矩阵和单位矩阵相乘都不会改变。我们将保持n维向量不变的单位矩阵记作:$$I_n\in\R^{n\times n}$$单位矩阵沿主对角线的元素都为1，其他都为0。
$$\begin{bmatrix}
1&0&0\\
0&1&0\\
0&0&1\\
\end{bmatrix}
$$
矩阵$A$的矩阵逆记作$A^{-1}$，满足以下条件：$$A^{-1}A=I_n$$现在我们可以通过以下步骤求解式(1):$$
\begin{aligned}
Ax=b\\
A^{-1}Ax=A^{-1}b\\
I_nx=A^{-1}b\\
x=A^{-1}b
\end{aligned}
$$可以看到，当逆矩阵$A^{-1}$存在时，通过上述办法可以求出x。
如果逆矩阵存在，那么对于式(1)每一个向量$b$恰好有一个解。但是对于方程组而言，向量$b$有可能无解，也有可能无限多个解，不可能有大于1个的有数解。
要使矩阵可逆，需要保证矩阵为方阵且所有列向量都是线性无关的，一个列向量线性相关的方阵被称为**奇异的**(singular)。

## 1.4 范数
在机器学习中，我们经常使用范数(norm)的函数来衡量向量大小。定义如下:$$||x||_p=(\sum_i|x_i|^p)^{\frac{1}{p}}$$其中$p\in\R$，$p\geq1$。当$p=1$时，$L^1$称为曼哈顿范数，$p=2$时，$L^2$称为欧几里得范数。在机器学习中平方$L^2$范数出现的非常频繁，可以通过点积计算$x^Tx$。

## 1.5 特殊类型的矩阵和向量

有些特殊类型的矩阵和向量是特别有用的。
**对角矩阵**(diagonal matrix) 只在主对角线上含有非零元素。
我们使用diag(v)表示对角元素由向量v中元素给定的一个对角方阵。对角矩阵和向量乘法满足：
$$diag(v)x=v\odot x$$对角方阵仅当对角元素都是非零值时逆矩阵存在，这时$$diag(v)^{-1}=diag([1/v_1,\cdots,1/v_n]^T)$$在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法，但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的算法。
**对称**(symmetric)矩阵是转置和自己相等的矩阵$A=A^T$。
**单位向量**(unit vector)是具有单位范数(unit norm)的向量，即$||x||_2=1$。如果$x^Ty=0$，那么向量$x$和$y$相互正交(orthogonal)。如果两个向量都有非零范数，那么这两个向量之间的夹角是90。在$\R^n$中至多有n个范数非零向量相互正交。如果这些向量相互正交且范数都为1，那么我们称它们为标准正交(orthonormal)。
**正交矩阵**(orthogonal matrix)指行向量和列向量是分别标准正交的方阵，即$$A^TA=AA^T=I$$这就意味着$A^{-1}=A^T$正交矩阵收到关注是因为求逆计算代价小。
## 1.6 特征分解（EVD）

特征分解是使用最广的矩阵分解之一，即我们将矩阵分解为一组特征向量和特征值。
$$Av=\lambda v$$
假设矩阵$A$有n个线性无关的特征向量$\{v^{(1)},\cdots,v^{(n)}\}$，对应着特征值$\{\lambda_1,\cdots,\lambda_n\}$。将这些特征向量连接成一个矩阵，使得每一列是一个特征向量$V=[v^{(1)},\cdots,v^{(n)}]$。类似的也将特征值连接成一个向量$\lambda=[\lambda_1,\cdots,\lambda_n]^T$。因此$A$的特征分解(eigendecomposition)可以记作：$$A=Vdiag(\lambda)V^{-1}$$每一个实对称矩阵都可以分解为实特征向量和实特征值：$$A=Q\Lambda Q^T\tag{2}$$其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。
所有特征值都是正数的矩阵称为正定(positive definite)；所有特征值都是非负数的矩阵称为半正定(positive semidefinite)。正定矩阵保证了$x^TAx=0\to x=0$。

## 1.7 奇异值分解（SVD）

奇异值分解(singular value decomposition ,SVD)是将矩阵分解为奇异向量(singular vector)和奇异值(singular value)。每一个实数矩阵都有一个奇异值分解，但不一定有特征分解，这时我们只能使用奇异值分解。
奇异值分解是将矩阵$A$分为三个矩阵的乘积：$$A=UDV^T$$假设$A$是一个mxn的矩阵，那么$U$是一个mxm的矩阵，$D$是一个mxn的矩阵，$V$是一个nxn的矩阵。矩阵$U$、$V$都定义为正交矩阵，而矩阵$D$定义为对角矩阵。$D$对角线上的元素称为$A$的奇异值。$U$的列向量称为左奇异向量，$V$的列向量称为右奇异向量。
事实上，$A$的左奇异向量是$AA^T$的特征向量，$A$的右奇异向量是$A^TA$的特征向量，$A$的非零奇异值是$A^TA$特征值的平方根，也是$AA^T$的平方根。

## 1.8 Moore-Penrose伪逆
对于非方阵不存在逆矩阵。我们希望通过矩阵$A$的左逆$B$来求解线性方程：$$Ax=y$$等式两边左乘左逆$B$后，我们得到$$x=By$$如果矩阵 A 的行数大于列数，那么上述方程可能没有解。如果矩阵 A 的行数小于列数，那么上述矩阵可能有多个解。
**Moore-Penrose**伪逆(Moore-Penrose pseudoinverse)计算伪逆的公式如下：$$A^+=VD^+U^+$$其中，矩阵$U$、$D$和$V$是矩阵A奇异值分解后得到的矩阵。对角矩阵D的伪逆$D^+$是非零元素倒数之后再转置得到的。当矩阵$A$的列多于行时，伪逆解法有$$x=A^+y$$是方程所有可行解中欧几里得范数$||x||_2$最小的一个。当矩阵$A$的行多于列时，可能没有解。这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离$||Ax-y||_2$最小。

## 1.9 迹运算

迹运算返回的是矩阵对角元素的和：$$Tr(A)=\sum_iA_{i,i}$$即使循环置换后矩阵乘积得到的矩阵形状改变了，迹运算的结果依然不变。$A\in \R^{m\times n}$，$B\in \R^{m\times n}$，可以得到$$Tr(AB)=Tr(BA)$$

## 1.10 主成分分析

主成分分析(principal components analysis, PC A)是一个简单的机器学习算法，可以通过基础的线性代数知识推导。
假设在$\R^n$空间中m个点$\{x^{(1)},\cdots,x^{(m)}\}$，我们希望对这些点进行有损压缩，我们希望损失的精度尽可能的小。对于原点$x^{(i)}\in \R^n$，映射点$c^{(i)}\in R^l$，我们需要使用矩阵乘法将编码映射为$$g(c)=Dc$$其中$D\in \R^{n\times l}$是定义的解码矩阵。为了使问题有唯一解，我们限制$D$中的素有列向量都有单位范数。
为了将这个基本想法变为我们能够实现的算法，首先我们需要明确如何根据每一个输入 x 得到一个最优编码$c^*$ 。一种方法是最小化原始输入向量 x 和重构向量$g(c^*)$之间的距离。我们使用范数来衡量它们之间的距离。在PCA算法中，我们使用L2范数
$$c^*=\arg\min_c||x-g(c)||_2$$
我们可以用平方L2范数代替L2范数，因为两者在相同值上c上取得最小值。这是因为L2范数是非负的，并且平方运算在非负值上是单调递增的。$$c^*=\arg\min_c||x-g(c)||_2^2$$
该最小化函数可以简化为
$$\begin{aligned}
&(x-g(c))^T(x-g(c))\\
&=x^Tx-x^Tg(c)-g(c)^Tx+g(c)^Tg(c)\\
&=x^tx-2x^Tg(c)+g(c)^Tg(c)
\end{aligned}$$
因为第一项$x^Tx$不依赖于c，可以忽略掉，得到如下优化的目标：
$$c^*=\arg\min_c -2x^Tg(c)+g(c)^Tg(c)$$
代入$f(c)$的定义：
$$\begin{aligned}
c^*&=\arg\min_c-2x^TDc+c^TD^TDc\\
&=\arg\min_c-2x^TDc+C^TI_lc\\
&=\arg\min_c-2x^TDc+c^Tc
\end{aligned}$$上式由矩阵D的正交性和单位范数约束可得。可以通过向量微积分来求解这个最优化问题。
$$\begin{aligned}
\nabla_c(-2x^TDc+c^Tc)=0\\
-2D^Tx+2c=0\\
c=D^Tx
\end{aligned}$$
这使得算法很高效：最优编码x只需要一个矩阵-向量乘法操作。我们使用编码函数:
$$f(x)=D^Tx$$
$$r(x)=g(f(x))=DD^Tx$$
接下来，我们需要挑选编码矩阵D。
$$D^*=\arg\min_D\sqrt{\sum_{i,j}(x_j^{(i)}-r(x^{(i)})_j)^2}$$受到$D^TD=I_l$约束。
最终得到$$\arg\max_d Tr(D^TX^TXd)$$受到$d^Td=1$约束。具体来说就是最优的d是$X^TX$最大特征值对应的特征向量。