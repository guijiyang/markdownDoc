# cuda执行配置优化

实现高性能的一个关键点是让设备的多处理器们尽量处在忙碌的状态.如果一个设备中多处理器的工作缺少平衡将会导致性能较低.因此,在设计应用程序时，一定要以最大限度地提高硬件利用率的方式使用线程和块，并限制妨碍工作自由分配的实践.

## 1. 占用(Occupancy)
cuda中的线程指令是顺序执行的,所以当一个warp暂停或者拖延时切换到另一个warp是保证硬件始终处于忙碌并隐藏延迟的唯一方式.因此,多处理器上面有多少个活动的warp决定硬件忙碌的效率高低.这就是occupancy.
occupancy是多处理器中活动warp和总warp的比值.

### 1.1. 计算occupancy
决定occupancy的几个因素之一是寄存器的可用性.线程的寄存器存储保持了临近的本地变量在访问时有低延迟性.然而,寄存器集是有限的资源需要多处理器上的所有线程共享.多个寄存器是一次性分配给一个block.所以如果每个block使用了很多的寄存器,多处理器上面可以存住的block数量将会减少,进而导致降低多处理器的occupancy.每个线程可以利用的最大寄存器数量可以手动设置,通过在编译时设置per-file的选项-maxrregcount或者per-kernel选项__launch_bounds__限定符.

为了计算occupancy,每个线程所用的寄存器数量是一个关键因素.例如在计算能力7.0的设备上每个多处理器有65536个32位寄存器并且最多有2048个线程同时存住(64warps*32threads per warp).这就表示对于这样的设备中的一个多处理器最多有32个寄存器. 但是这种确定寄存器计数如何影响占用的方法没有考虑寄存器分配粒度.比如在一个计算能力7.0的设备上,一个kernel中每个block有128个线程,每个线程中有37个寄存器,这样推算出多处理器的占用率为75%(12个激活block).当一个kernel中每一个block有320个线程,每一个线程有37个寄存器时,这样推算出多处理器的占用率约等于63%(4个激活block).

nvcc的 --ptxas  option=v选项详细说明每个内核每个线程使用的寄存器数量.

## 2. 隐藏寄存器依赖(Hiding Register Dependencies)

寄存器依赖是指令调用前一个指令写入的寄存器数据结果引起的.在计算能力7.0的设备上大多数算术函数的延迟通常是4个时钟时间.所以线程在使用算术结果之前需要等待大概4个时钟时间.然后,这个延迟可以通过切换到其他线程在隐藏.

## 3. 线程和区块的启发式教育(Thread and Block Heuristics)

延迟隐藏和occupancy依赖于多处理器中的活动warp数量.而活动的warp数量又隐式的取决于kernel的执行参数和设备资源(共享内存和寄存器)限制.选择执行参数需要在延迟隐藏(占用)和资源利用率之间取得平衡。

在选择kernel第一个执行配置参数(每个网格的块数或网格大小)时，主要考虑的是保持整个GPU繁忙。网格中的块的数量应该大于多处理器的数量，以便所有多处理器至少有一个块要执行。此外，每个多处理器应该有多个活动块，这样那些没有等待__syncthreads()的块就可以使硬件保持繁忙状态。该建议视资源的可得性而定;因此，它应该在第二个执行参数(每个块的线程数或块大小)和共享内存使用情况的上下文中确定。要扩展到未来的设备，每个内核启动的块数应该是数千。

在选择块大小时，一定要记住，多个并发块可以驻留在一个多处理器上，因此占用率不是仅由块大小决定的。特别是，较大的块尺寸并不意味着更高的occupancy。

正如在occupancy中提到的，更高的占用率并不总是意味着更好的性能。例如，将占用率从66%提高到100%，通常不会带来类似的性能增长。占用率较低的内核比占用率较高的内核在每个线程上有更多的可用寄存器，这可能会导致较少的寄存器内核临时变量溢出到本地内存;特别是，使用高度公开的指令级并行(ILP)，在某些情况下，可以用低占用率完全覆盖延迟。

在选择块大小时涉及许多这样的因素，不可避免地需要一些实验。然而，应该遵循一些经验法则:

- block中线程数应当是warp尺寸的成倍数以避免未充分填充的warp浪费计算资源并有利于合并.
- 每个block中最少要有64个线程,且是在多处理器中有许多并行blocks的情况下.
- 对应不同的block数量block中128-256个线程是比较合适的起始实验范围.
- 如果延迟表现较好时可选择多个小的线程块代替一个大块.特别是在kernel经常调用__syncthreads()的情况下非常有益.

需要注意的是当线程块需要的寄存器数量超过处理器的可能提供的数量,将导致kernel启动失败.当请求过多共享内存或线程时也是如此.

## 4. 共享内存的效果(Effects of Shared Memory)

共享内存在某些情况下是有用的，比如帮助合并或消除对全局内存的冗余访问。然而，它也可以作为占用的约束。
要确定性能对占用的敏感性，一种有用的技术是通过动态分配共享内存的数量来进行实验，如执行配置的第三个参数中所指定的那样。通过简单地增加这个参数(不修改内核)，就可以有效地减少内核的占用并度量它对性能的影响。

## 5. 核并行执行(Concurrent Kernel Execution)

正如在异步和与计算重叠传输中所描述的，CUDA流可用于将内核执行与数据传输重叠。

## 6. 多上下文(Multiple contexts)

发生在进程空间的一个特定的GPU的CUDA工作称为上下文。上下文封装了该GPU的内核启动和内存分配，以及支持结构(如页表)。上下文在CUDA驱动程序API中是显式的，但在CUDA运行时API中是完全隐式的，它自动创建和管理上下文。

使用CUDA驱动程序API, CUDA应用程序进程可以为一个给定的GPU创建多个上下文。如果多个CUDA应用进程同时访问同一GPU，这几乎总是意味着多个上下文，因为一个上下文绑定到一个特定的主机进程，除非使用多进程服务。

虽然可以在一个给定的GPU上并发地分配多个上下文(以及它们的相关资源，如全局内存分配)，但是在该GPU上，只有一个上下文可以在任何给定的时刻执行工作.一个GPU上面的上下文是通过时间片共享的.创建额外的上下文需要上下文数据的内存开销和上下文切换的时间开销.此外，当来自多个上下文的工作可以同时执行时，对上下文切换的需求可能会降低利用率.

因此，最好避免在同一个CUDA应用程序的每个GPU的同时存在多个上下文。为了帮助实现这一点，CUDA Driver API提供了一些方法来访问和管理每个GPU上的一个特殊上下文，这个特殊上下文被称为主上下文。这些和cuda运行时thread没有一个当前上下文时隐式使用的上下文相同.

```c++
// When initializing the program/library
CUcontext ctx;
cuDevicePrimaryCtxRetain(&ctx, dev);
// When the program/library launches work
cuCtxPushCurrent(ctx);
kernel<<<...>>>(...);
cuCtxPopCurrent(&ctx);
// When the program/library is finished with the context
cuDevicePrimaryCtxRelease(dev);
```