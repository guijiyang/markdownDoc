> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.cnblogs.com](https://www.cnblogs.com/jokerjason/p/10711022.html)

CPU 体系结构之 cache 小结

**1.What is cache?**

**Cache 是用来对内存数据的缓存。**

**CPU 要访问的数据在 Cache 中有缓存，称为 “命中” (Hit)，反之则称为 “缺失” (Miss)。**

**CPU 访问它的速度介于寄存器与内存之间（数量级的差别）。实现 Cache 的花费介于寄存器与内存之间。**

现在 CPU 的 Cache 又被细分了几层，常见的有 L1 Cache, L2 Cache, L3 Cache，其读写延迟依次增加，实现的成本依次降低。

现代系统采用从 Register ―> L1 Cache ―> L2 Cache ―> L3 Cache ―> Memory ―> Mass storage 的层次结构，是为解决性能与价格矛盾所采用的折中设计。

下图描述的就是 CPU、Cache、内存、以及 DMA 之间的关系。程序的指令部分和数据部分一般分别存放在两片不同的 cache 中，对应指令缓存（I-Cache）和数据缓存（D-Cache）。

 ![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415153854116-1717451517.png)

引入 Cache 的理论基础是**程序局部性原理**，包括时间局部性和空间局部性。即最近被 CPU 访问的数据，短期内 CPU 还要访问（时间）；被 CPU 访问的数据附近的数据，CPU 短期内还要访问（空间）。因此如果将刚刚访问过的数据缓存在 Cache 中，那下次访问时，可以直接从 Cache 中取，其速度可以得到数量级的提高。

  
CPU 缓存（Cache Memory）位于 CPU 与内存之间的临时存储器，它的容量比内存小但交换速度快。在缓存中的数据是内存中的一小部分，但这一小部分是短时间内 CPU 即将访问的，当 CPU 调用大量数据时，就可避开内存直接从缓存中调用，从而加快读取速度。  
在 CPU 中加入缓存是一种高效的解决方案，这样整个内存储器（缓存 + 内存）就变成了既有缓存的高速度，又有内存的大容量的存储系统了。缓存对 CPU 的性能影响很大，主要是因为 CPU 的数据交换顺序和 CPU 与缓存间的带宽引起的。  
下图是一个典型的存储器层次结构，我们可以看到一共使用了三级缓存  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415151355245-1616621096.png)

2. Why should I care about cache?  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415151438010-85846939.png)  
从延迟上看，做一次乘法一般只要三个周期，而做一次 CPU 的内存访问需要 167 个 cycle，如果需要提升程序性能，减少 CPU 的 memory 访问至关重要。因此，需要采用容量小但是更快的存储器（cache）。

3. 为什么要有多级 CPU Cache  
随着科技发展，热点数据的体积越来越大，单纯的增加一级缓存大小的性价比已经很低了  
二级缓存就是一级缓存的缓冲器：一级缓存制造成本很高因此它的容量有限，二级缓存的作用就是存储那些 CPU 处理时需要用到、一级缓存又无法存储的数据。  
同样道理，三级缓存和内存可以看作是二级缓存的缓冲器，它们的容量递增，但单位制造成本却递减。  
另外需要注意的是，L3 Cache 和 L1，L2 Cache 有着本质的区别。，L1 和 L2 Cache 都是每个 CPU core 独立拥有一个，而 L3 Cache 是几个 Cores 共享的，可以认为是一个更小但是更快的内存。  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415151555484-995491025.png)  
使用 dmidecode 命令查看 cache size  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415151607267-286721434.png)

4.cpu 与 cache 内存交互的过程  
CPU 接收到指令后，它会最先向 CPU 中的一级缓存（L1 Cache）去寻找相关的数据，然一级缓存是与 CPU 同频运行的，但是由于容量较小，所以不可能每次都命中。这时 CPU 会继续向下一级的二级缓存（L2 Cache）寻找，同样的道理，当所需要的数据在二级缓存中也没有的话，会继续转向 L3 Cache、内存 (主存) 和硬盘.  
程序运行时可以使用 perf 工具观察 cache-miss 的 rate.

  
5. 什么是 cache line  
Cache Line 可以简单的理解为 CPU Cache 中的最小缓存单位。  
内存和高速缓存之间或高速缓存之间的数据移动不是以单个字节或甚至 word 完成的。  
相反，移动的最小数据单位称为缓存行，有时称为缓存块  
目前主流的 CPU Cache 的 Cache Line 大小都是 64Bytes。假设我们有一个 512 字节的一级缓存，那么按照 64B 的缓存单位大小来算，这个一级缓存所能存放的缓存个数就是 512/64 = 8 个。  
查看 cache line 大小  
cat /sys/devices/system/cpu/cpu1/cache/index0/coherency_line_size  
cache line 的影响：

```
for (int i = 0; i < N; i+=k)
    arr[i] *= 3;

```

![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415151723631-1714332519.png)  
注意当步长在 1 到 16 范围内，循环运行时间几乎不变。但从 16 开始，每次步长加倍，运行时间减半。  
由于 16 个整型数占用 64 字节（一个缓存行），for 循环步长在 1 到 16 之间必定接触到相同数目的缓存行：即数组中所有的缓存行。当步长为 32，我们只有大约每两个缓存行接触一次，当步长为 64，只有每四个接触一次。

**6. cache 写机制**  
Cache 写机制分为 write through 和 write back 两种。  
Write-through- Write is done synchronously both to the cache and to the backing store.  
Write-back (or Write-behind) - Writing is done only to the cache. A modified cache block is written back to the store, just before it is replaced.  
Write-through（直写模式）在数据更新时，同时写入缓存 Cache 和后端存储。此模式的优点是操作简单；缺点是因为数据修改需要同时写入存储，数据写入速度较慢。  
Write-back（回写模式）在数据更新时只写入缓存 Cache。只在数据被替换出缓存时，被修改的缓存数据才会被写到后端存储。此模式的优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回。

**7.cache 一致性**  
多个处理器对某个内存块同时读写，会引起冲突的问题，这也被称为 Cache 一致性问题。  
Cache 一致性问题出现的原因是在一个多处理器系统中，多个处理器核心都能够独立地执行计算机指令，从而有可能同时对某个内存块进行读写操作，并且由于我们之前提到的回写和直写的 Cache 策略，导致一个内存块同时可能有多个备份，有的已经写回到内存中，有的在不同的处理器核心的一级、二级 Cache 中。由于 Cache 缓存的原因，我们不知道数据写入的时序性，因而也不知道哪个备份是最新的。还有另外一个一种可能，假设有两个线程 A 和 B 共享一个变量，当线程 A 处理完一个数据之后，通过这个变量通知线程 B，然后线程 B 对这个数据接着进行处理，如果两个线程运行在不同的处理器核心上，那么运行线程 B 的处理器就会不停地检查这个变量，而这个变量存储在本地的 Cache 中，因此就会发现这个值总也不会发生变化。  
为了正确性，一旦一个核心更新了内存中的内容，硬件就必须要保证其他的核心能够读到更新后的数据。目前大多数硬件采用的策略或协议是 MESI 或基于 MESI 的变种：  
M 代表更改（modified），表示缓存中的数据已经更改，在未来的某个时刻将会写入内存；  
E 代表排除（exclusive），表示缓存的数据只被当前的核心所缓存；  
S 代表共享（shared），表示缓存的数据还被其他核心缓存；  
I 代表无效（invalid），表示缓存中的数据已经失效，即其他核心更改了数据。  
8.cache 的局部性  
程序在一段时间内访问的数据通常具有局部性，比如对一维数组来说，访问了地址 x 上的元素，那么以后访问地址 x+1、x+2 上元素的可能性就比较高；现在访问的数据，在不久之后再次被访问的可能性也比较高。局部性分为 “时间局部性” 和“空间局部性”，时间局部性是指当前被访问的数据随后有可能访问到；空间局部性是指当前访问地址附近的地址可能随后被访问。处理器通过在内存和核心之间增加缓存以利用局部性增强程序性能，这样可以用远低于缓存的价格换取接近缓存的速度。  
**时间局部性：**  
代码 1：  

```
for (loop=0; loop<10; loop++) {
    for (i=0; i<N; i++) {
        ... = ... x[i] ...
    }
}

```

  
代码 2：  

```
for (i=0; i<N; i++) {
    for (loop=0; loop<10; loop++) {
        ... = ... x[i] ...
    }
}

```

  
代码二的性能优于代码 1，x 的元素现在被重复使用，因此更有可能留在缓存中。 这个  
重新排列的代码在使用 x[i] 时显示更好的时间局部性。  
**空间局部性：**  
一个矩阵乘法的例子：  
代码 1：

```
for i=1..n
    for j=1..n
        for k=1..n
            c[i,j] += a[i,k]*b[k,j]

```

  
代码 2：

```
for i=1..n
    for k=1..n
        for j=1..n
            c[i,j] += a[i,k]*b[k,j]

```

  
代码 2 的性能优于代码一的性能。  
两者实现上的差异：  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415151935721-708823313.png)  
代码 2 的 b[k,j] 是按行访问的，所以存在良好的空间局部性，cache line 被充分利用。  
代码 1 中，b [k，j] 由列访问。 由于行的存储矩阵，因此对于每个缓存行加载，只有一个元素用于遍历。

  
9.cache 替换策略  
Cache 工作原理要求它尽量保存最新数据，当从主存向 Cache 传送一个新块，而 Cache 中可用位置已被占满时，就会产生 Cache 替换的问题。  
常用的替换算法有下面三种。  
（1） LFU  
LFU（Least Frequently Used，最不经常使用）算法将一段时间内被访问次数最少的那个块替换出去。每块设置一个计数器，从 0 开始计数，每访问一次，被访块的计数器就增 1。当需要替换时，将计数值最小的块换出，同时将所有块的计数器都清零。  
这种算法将计数周期限定在对这些特定块两次替换之间的间隔时间内，不能严格反映近期访问情况，新调入的块很容易被替换出去。  
（2）LRU  
LRU（Least Recently Used，近期最少使用）算法是把 CPU 近期最少使用的块替换出去。这种替换方法需要随时记录 Cache 中各块的使用情况，以便确定哪个块是近期最少使用的块。每块也设置一个计数器，Cache 每命中一次，命中块计数器清零，其他各块计数器增 1。当需要替换时，将计数值最大的块换出。  
LRU 算法相对合理，但实现起来比较复杂，系统开销较大。这种算法保护了刚调入 Cache 的新数据块，具有较高的命中率。LRU 算法不能肯定调出去的块近期不会再被使用，所以这种替换算法不能算作最合理、最优秀的算法。但是研究表明，采用这种算法可使 Cache 的命中率达到 90% 左右。  
（3） 随机替换  
最简单的替换算法是随机替换。随机替换算法完全不管 Cache 的情况，简单地根据一个随机数选择一块替换出去。随机替换算法在硬件上容易实现，且速度也比前两种算法快。缺点则是降低了命中率和 Cache 工作效率。

**10.cache 的映射**  
主存与 cache 的地址映射方式有全相联方式、直接方式和组相联方式三种。  
直接映射  
将一个主存块存储到唯一的一个 Cache 行。  
1) 多对一的映射关系，但一个主存块只能拷贝到 cache 的一个特定行位置上去。  
cache 的行号 i 和主存的块号 j 有如下函数关系：i=j mod m（m 为 cache 中的总行数）  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415152015554-2116413964.png)  
优点：硬件简单，容易实现  
缺点：命中率低， Cache 的存储空间利用率低  
2) 全相联映射  
可以将一个主存块存储到任意一个 Cache 行。  
主存的一个块直接拷贝到 cache 中的任意一行上  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415152038861-573924176.png)  
优点：命中率较高，Cache 的存储空间利用率高  
缺点：线路复杂，成本高，速度低  
组相联映射  
可以将一个主存块存储到唯一的一个 Cache 组中任意一个行。  
将 cache 分成 u 组，每组 v 行，主存块存放到哪个组是固定的，至于存到该组哪一行是灵活的，即有如下函数关系：cache 总行数 m＝u×v 组号 q＝j mod u  
![](https://img2018.cnblogs.com/blog/941117/201904/941117-20190415152136772-1526893524.png)  
组间采用直接映射，组内为全相联  
硬件较简单，速度较快，命中率较高  

Cache Miss

1. 不要期望编译器对你做任何优化

2.Cache(广义内存) 系统代表性的包括三种级别：  
（1）第一级 cache (L1) 位于 CPU 芯片上并且运算于 CPU 工作频率；  
（2）第二级 cache(L2) 也位于芯片上比 L1 速度慢而体积大；  
（3）第三级 cache(L3) 位于 CPU 外部，是速度最慢、体积最大的存储器。

3. 当运算器需要从存储器中提取数据时，它首先在最高级的 cache 中寻找然后在次高级的 cache 中寻找。如果在 cache 中找到，则称为命中 hit；反之，则称为不命中 miss。

4.cache misses 的种类：

（1）cold misses：不可避免。若 K 级 cache 空，则必发生 cache miss，空的 cache 称为 cold cache，这种 cache misses 称为 compulsory misses 或者 cold misses。当 cache 已被 warmed up 则一般不会再发生 cold misses。

（2）conflict misses：多个 K+1 级的 blocks 被映射到 K 级中同一个 block。这一点关系到对于程序员而言能否写出 cache 友好代码。

（3）程序常会分阶段执行（例如循环：内层、外层），每个阶段会取 cache blocks 的固定几个块，这几个块所构成的集合称为 working set。 当 working set 超过 cache 大小时所发生的 miss 称为 capacity misses。

5. 从 cache 指令上做优化：简化调用关系，减少冗余代码（即不是必须存在的的代码），减小代码量，减少不必要的调用；

6. 从数据 cache 上做优化：即减少 cache miss 的次数，方法有不少，http://blog.chinaunix.net/uid-7319742-id-2059720.html 这篇文章有介绍

推荐链接：

http://bi.dataguru.cn/thread-163962-1-1.html

http://blog.chinaunix.net/uid-7319742-id-2059720.html

http://blog.csdn.net/wangjiaoyu250/article/details/9212863

http://coolshell.cn/articles/10249.html

https://blog.csdn.net/yhb1047818384/article/details/79604976